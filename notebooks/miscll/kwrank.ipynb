{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "#import textract\n",
    "#text = textract.process(\"C:/Users/Ranja.Sarkar/Desktop/new1/GS.05.51610_Polyethylene lined pipe systems.pdf\")\n",
    "\n",
    "#path = r\"C:/Users/Ranja.Sarkar/Desktop/new1/GS.05.51610_Polyethylene lined pipe systems.pdf\"\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "pdfFileObj = open('C:/Users/Ranja.Sarkar/Desktop/new3/GS.05.51610_Polyethylene lined pipe systems.pdf','rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "pageObj = pdfReader.getPage(9)\n",
    "pagetxt = pageObj.extractText()\n",
    "#print(pagetxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPACY FOR POS TAGGING\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "#python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text,token.lemma_,token.pos_,token.is_stop)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(pagetxt)\n",
    "#doc = nlp(\"Environmental Stress Cracking (ESC) can best be described as the localized cleaving of portions of molecular chains in an area of concentrated stress. Fluids that will not usually attack a polymer in an unstressed state will attack the area weakened by localized stress causing a crack or craze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for sents in doc.sents:\n",
    "#    print(len(sents.text))\n",
    "    print(sents.text)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##words in the sentences, considered with POS tags only\n",
    "\n",
    "\"\"\"\n",
    "words_pos = ['NOUN','PROPN','VERB','ADJECTIVE','ADVERB']\n",
    "\n",
    "sentences = []\n",
    "for sent in doc.sents:\n",
    "    selected_words = []\n",
    "    for token in sent:\n",
    "        if token.pos_ in words_pos and token.is_stop is False:\n",
    "            selected_words.append(token)\n",
    "    sentences.append(selected_words)\n",
    "print(sentences)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 #damping coefficient\n",
    "        self.min_diff = 1e-5 #convergence threshold\n",
    "        self.steps = 20 #iteration steps, not 10\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "        \n",
    "        \n",
    "    def set_stopwords(self,stopwords):\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "            \n",
    "    def sentence_segment(self,doc,words_pos,lower):\n",
    "        \"\"\"Store words only in words_pos tag\"\"\"\n",
    "        sentences=[]\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                if token.pos_ in words_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return(sentences)\n",
    "\n",
    "    def get_vocab(self,sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "\n",
    "        return token_pairs\n",
    "    \n",
    "    def symmetrize(self, a):\n",
    "\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "\n",
    "\n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "         \n",
    "\n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "\n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is to ignore 0 element in norm\n",
    "\n",
    "        return g_norm\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "    \n",
    "    def analyze(self, text, words_pos = ['NOUN', 'PROPN', 'VERB','ADJECTIVE','ADVERB'], window_size = 8, lower = False, stopwords = list()):\n",
    "        \n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "      \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "\n",
    "        # Parse text by spaCy\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, words_pos, lower) # list of list of words\n",
    "     \n",
    "\n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "\n",
    "        \n",
    "        # Initialization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "\n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "\n",
    "        self.node_weight = node_weight  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liner - 4.489944934035321\n",
      "ESC - 3.285848979487741\n",
      "lined - 3.2811066548079455\n",
      "liners - 2.4939643830089597\n",
      "failures - 2.4197970364333306\n",
      "PE - 2.234195475859875\n",
      "pipelines - 2.2236345716529633\n",
      "Bar - 1.9498810491276397\n",
      "collapse - 1.7734945969762803\n",
      "Table - 1.6771637971173572\n",
      "localized - 1.6130258908830464\n",
      "Shell - 1.5956041715468787\n"
     ]
    }
   ],
   "source": [
    "text = pagetxt\n",
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, words_pos = ['NOUN', 'PROPN','VERB','ADJECTIVE','ADVERB'], window_size=8, lower=False)\n",
    "#tr4w.get_keywords(10)\n",
    "kw = tr4w.get_keywords()\n",
    "#table = pd.DataFrame(kw,columns=['weights','keywords'])\n",
    "#table = table.sort_values('weights',ascending=False)\n",
    "#table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "from rake_nltk import Rake\n",
    "import pandas as pd\n",
    "\n",
    "r = Rake()\n",
    "\n",
    "words = r.extract_keywords_from_text(text)\n",
    "#phrases = r.get_ranked_phrases()\n",
    "phrases = r.get_ranked_phrases_with_scores()\n",
    "table = pd.DataFrame(phrases,columns=['score','Key phrases'])\n",
    "table = table.sort_values('score',ascending=False)\n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENSIM FOR KEYWORD EXTRACTION\n",
    "\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import pandas as pd\n",
    "\n",
    "text = pagetxt\n",
    "values = keywords(text, split = '\\n',scores = True)\n",
    "#data = pd.DataFrame(values,columns=['Keywords','Score'])\n",
    "#data = data.sort_values('Score',ascending=False)\n",
    "#data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
